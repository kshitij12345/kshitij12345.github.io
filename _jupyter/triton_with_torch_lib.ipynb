{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJFvgqcj1Z3P"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "PyTorch is one of the prominent Deep Learning Frameworks. It ships with a lot of useful operators and modules for building and training your own Deep Learning model. However, in certain cases, you can write your own CUDA kernels for certain operations to improve performance. \n",
        "\n",
        "In this blog, we are going to try `Triton`, a Python Framework, which helps us to write efficient CUDA kernels at runtime and use them to improve our performance. We will then use this `Triton` kernel to override one of the existing operator kernel in PyTorch. Thus we don't have to write our model code any different (from the PyTorch API) and yet we will see performance boost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Triton\n",
        "%pip install triton==2.0.0.dev20221005\n",
        "# Install PyTorch\n",
        "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuvdXXt6Q8j1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2KGveT3Ydkq"
      },
      "source": [
        "### Enter Triton\n",
        "\n",
        "Triton is a language and compiler which enables us as Python user to write and generate blazingly fast CUDA kernels at run-time and use them. Writing an efficient GPU kernel is not a trivial task as one has to ensure that memory and compute are being utilized efficiently.\n",
        "\n",
        "This [blog](https://openai.com/blog/triton/) walks us through the idea and architecture of Triton."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHqf_cgOWmKI"
      },
      "source": [
        "#### Writing a CUDA kernel for `sin` using Triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z7MhkvPQ_W7"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# `sin` CUDA kernel\n",
        "@triton.jit\n",
        "def _sin(x_ptr, output_ptr,\n",
        "        BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(axis=0)\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    # Load data\n",
        "    x = tl.load(x_ptr + offsets)\n",
        "    output = tl.sin(x)\n",
        "    # Write-back output\n",
        "    tl.store(output_ptr + offsets, output)\n",
        "\n",
        "\n",
        "def triton_sin(x):\n",
        "    output = torch.empty_like(x)\n",
        "    assert x.is_contiguous()\n",
        "    n_elements = x.numel()\n",
        "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
        "    # Launch `sin` kernel\n",
        "    _sin[grid](x, output, BLOCK_SIZE=256)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06zucr0koOuI"
      },
      "source": [
        "Check for simple correctness of our kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDcNPG_ShfSF"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Input tensor\n",
        "x = torch.randn(size=(1024, 1024)).cuda()\n",
        "\n",
        "# Verify that our outputs match with the PyTorch's output\n",
        "torch.testing.assert_close(triton_sin(x), torch.sin(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjZoPFz0YLtm"
      },
      "source": [
        "#### Benchmark `torch` implementation vs Triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD-RivInZn5A"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Helper for benchmarking\n",
        "# Benchmark `torch.sin` vs `triton_sin`\n",
        "def run_benchmark():\n",
        "    @triton.testing.perf_report(\n",
        "        triton.testing.Benchmark(\n",
        "            x_names=['N'],  # argument names to use as an x-axis for the plot\n",
        "            x_vals=[\n",
        "                128 * i for i in range(2, 100)\n",
        "            ],  # different possible values for `x_name`\n",
        "            line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
        "            line_vals=[\n",
        "                'triton',\n",
        "                'torch-native',\n",
        "            ],  # possible values for `line_arg``\n",
        "            line_names=[\n",
        "                \"Triton\",\n",
        "                \"Torch (native)\",\n",
        "            ],  # label name for the lines\n",
        "            styles=[('blue', '-'), ('green', '-')],  # line styles\n",
        "            ylabel=\"GB/s\",  # label name for the y-axis\n",
        "            plot_name=\"sin-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
        "            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
        "        )\n",
        "    )\n",
        "    def benchmark(M, N, provider):\n",
        "        x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n",
        "        if provider == 'torch-native':\n",
        "            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.sin(x))\n",
        "        if provider == 'triton':\n",
        "            ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_sin(x))\n",
        "        gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
        "        return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
        "\n",
        "    benchmark.run(show_plots=True, print_data=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "5YXCtQUqR-xq",
        "outputId": "e4073833-62b2-49a8-dda1-687d186088ee"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX9D9ICIpGIp"
      },
      "source": [
        "Based on the graph above, we can see that the Triton kernel has better performance than the native PyTorch kernel for the given input sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGXMhQ0qXUJQ"
      },
      "source": [
        "### Let's replace PyTorch's CUDA kernel with Triton Kernel\n",
        "\n",
        "#### Enter torch.library.Library\n",
        "\n",
        "Doc: https://pytorch.org/docs/stable/library.html?highlight=library#torch.library.Library\n",
        "\n",
        "PyTorch provides a mechanism to easily define and implement kernels for operators via Python. This also allow us to update `torch.sin` to always dispatch to the Triton generated kernel for tensor on CUDA device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_QDtjgfWB7S"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "lib = torch.library.Library(\"aten\", \"IMPL\", \"CUDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkQXDYm-Xmhr"
      },
      "source": [
        "#### We should get a warning notifying us that we are overriding an existing kernel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XLAnoi1XXm8",
        "outputId": "493b179f-6a81-4d84-d28c-2a9e3244f780"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Henceforth, all calls to `torch.sin` with CUDA tensor will be dispatched\n",
        "# to our Triton generated kernel. To verify, we should see no difference\n",
        "# between `torch.sin` vs `triton_sin` in terms of performance.\n",
        "lib.impl('sin', triton_sin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYZr5E99X4WO"
      },
      "source": [
        "### Re-run the benchmark, PyTorch native and triton version should have same performance now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "tDXNPOabXkHH",
        "outputId": "bcc8353e-d7b9-4774-dab7-e799f91c1060"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "run_benchmark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMXr5_sr21Xv"
      },
      "source": [
        "And we still get the automatic differentiation on the overridden operator for free"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3stw1m_93rbC"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "x = torch.randn(10, device='cuda', requires_grad=True)\n",
        "y = x.sin()\n",
        "y.sum().backward()\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.testing.assert_close(x.grad, x.cos())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsSAP4yG29Uq"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "* We saw that Triton allows us to write efficient CUDA kernels very easily in Python. \n",
        "* We also used `torch.library.Library` to override the PyTorch's implementation with ours. This way, we can just override the kernels and get performance boost on existing models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TgoI0d55WrZ"
      },
      "source": [
        "References:\n",
        "1. https://openai.com/blog/triton/\n",
        "2. https://triton-lang.org/master/index.html\n",
        "3. https://pytorch.org/docs/stable/library.html#torch-library"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
