<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://kshitij12345.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kshitij12345.github.io/" rel="alternate" type="text/html" /><updated>2022-01-17T15:02:19+05:30</updated><id>https://kshitij12345.github.io/feed.xml</id><title type="html">Hacker’s Getaway</title><subtitle>Rough slate</subtitle><author><name>Kshiteej</name><email>kshitijkalambarkar@gmail.com</email></author><entry><title type="html">JITerator</title><link href="https://kshitij12345.github.io/pytorch,/cuda/2022/01/17/Jiterator.html" rel="alternate" type="text/html" title="JITerator" /><published>2022-01-17T14:14:49+05:30</published><updated>2022-01-17T14:14:49+05:30</updated><id>https://kshitij12345.github.io/pytorch,/cuda/2022/01/17/Jiterator</id><content type="html" xml:base="https://kshitij12345.github.io/pytorch,/cuda/2022/01/17/Jiterator.html">&lt;p&gt;PyTorch is one of the leading Deep Learning framework. It supports a lot of operations to operate on Tensor.
This rich set of operators allow researchers to quickly proto-type new solutions to their problems. As these operators
are the basic building block of any PyTorch script, it is imperative that they are as performant as possible. Writing performant
operators with parallelization while keeping the cache hot is not easy. Writing these operator considering that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;s are not always laid out contiguously in memory, is certainly tricky. Handling these myraid of cases while maintaining performance does not sounds like a non-trivial development task.&lt;/p&gt;

&lt;h3 id=&quot;enter-the-tensoriterator&quot;&gt;Enter the TensorIterator!&lt;/h3&gt;
&lt;p&gt;PyTorch internals has a nice and friendly infrastructure to do the same. It helps take care of writing performant code while handling
all these edge cases. So all that the developer needs to add is the actual computation of the operator. When using TensorIterator,
the developer only has to specify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inputs&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outputs&lt;/code&gt; for the operator and how to transform those inputs to the output value.
TensorIterator also has extra checks and features to make sure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inputs&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outputs&lt;/code&gt; don’t overlap, they are on same the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;device&lt;/code&gt;, they have same the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dtype&lt;/code&gt;. It also takes care of type promotion if the correct flags are set.&lt;/p&gt;

&lt;p&gt;Example code using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TensorIterator&lt;/code&gt; (from first blog reference)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;at::TensorIteratorConfig iter_config;
iter_config
  .add_output(c)
  .add_input(a)
  .add_input(b);

auto iter = iter_config.build();

// Element-wise add
at::native::cpu_kernel(iter, [] (float a, float b) -&amp;gt; float {
  return a + b;
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A ton of PyTorch operators use TensorIterator internally. TensorIterator suits very well for elementwise operations like Unary Operators (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exp&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sin&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log&lt;/code&gt;, etc.) and Binary Operators (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mul&lt;/code&gt;, etc.). It also works well for Reduction Operators like (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mean&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prod&lt;/code&gt;, etc.).&lt;/p&gt;

&lt;p&gt;For more details on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TensorIterator&lt;/code&gt; one can refer the following blogs which get into the detail of how it works and have various examples
about it’s usage.&lt;/p&gt;

&lt;p&gt;Blogs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://labs.quansight.org/blog/2021/04/pytorch-tensoriterator-internals-update/index.html&lt;/li&gt;
  &lt;li&gt;https://labs.quansight.org/blog/2020/04/pytorch-tensoriterator-internals/&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tensoriterator-and-cuda&quot;&gt;TensorIterator and CUDA&lt;/h3&gt;

&lt;p&gt;The existing machinery for using TensorIterator to generate CUDA Kernels of the operation is really easy to use but there is one major caveat in the process. The helper functions like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cpu_kernel&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpu_kernel&lt;/code&gt; which generate the kernel code for the operator are heavily templated. These templates are instantiated for all the dtypes that are supported by the operator and if a binary operator supports scalar argument, then two more kernels are instantiated per dtype for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor-Scalar&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Scalar-Tensor&lt;/code&gt; case. This leads to a lot of increased build time especially for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvcc&lt;/code&gt; (CUDA AOT compiler). Also these kernels are compiled using CUDA Runtime API, which means that these kernels are loaded when PyTorch binary is loaded which increases the CUDA context size (which ends up consuming actual GPU VRAM). These issues became very apparent while adding new operators for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.special&lt;/code&gt; module. A general user who didn’t care about these operators had to actually pay the cost (in terms of GPU memory) for them when we loaded PyTorch with CUDA support.&lt;/p&gt;

&lt;h3 id=&quot;the-jiterator&quot;&gt;The JITerator&lt;/h3&gt;

&lt;p&gt;The solution to these problems that the PyTorch maintainers Mike Rubbery and Natalia Gimelshein came up with was to use NVRTC (Runtime Compilation) library shipped with CUDA to delay the compilation and loading of these kernels. Another way to put it is,
we will not compile the kernel and load the corresponing kernel binary till the operator is called for the first. This way there is not a heavy upfront compilation cost and the operator kernel code is loaded in the GPU memory only if the operator is actually used.&lt;/p&gt;

&lt;p&gt;The way this conceptually works is when PyTorch is built it keeps the string representation of the kernel code which needs to be compiled when an operator is called. So when a user actually calls the operator, we check in cache if there is already an kernel available, if not NVRTC is utilized to generate kernel and load it for use. The address of this generated kernel is cached so that next time when is operator is called, we can use this compiled kernel. We will expand this idea in the following section and dive deeper into JITerator.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How to use JITerator with TensorIterator
    &lt;ul&gt;
      &lt;li&gt;Computation String (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jiterator_stringify&lt;/code&gt;)&lt;/li&gt;
      &lt;li&gt;Using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Diving Deeper
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel_impl&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;NVRTC JIT utility helpers&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_unrolled_kernel&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_vectorized_kernel&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;computation-string&quot;&gt;Computation String&lt;/h4&gt;
&lt;p&gt;Let’s take a look at how the said string looks in code. We will look at the implementation of the binary operator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcd&lt;/code&gt;. Below is the code for computing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcd&lt;/code&gt; of two numbers &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a_in&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b_in&lt;/code&gt;.
Do note that we code written doesn’t look like string. This is because of the macro &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jiterator_stringify&lt;/code&gt; which converts the code to string using preprocessor capabilities. Thanks to this we don’t loose out on syntax highlighting and the code still feels like code even if we are actually getting the string of the code in the variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcd_string&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gcd_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jiterator_stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gcd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// gcd_string&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;using-jitted_gpu_kernel&quot;&gt;Using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Now that we have string representation of our computation, we need to setup the TensorIterator and pass it to the JITerator machinery which will hold onto to this computation string and compile it once the relevant operator is called.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Setting up the TensorIterator&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorIteratorConfig&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;iter_config&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Defining the kernel.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gcd_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;gcd&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gcd_kernel_cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorIteratorBase&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;AT_DISPATCH_INTEGRAL_TYPES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;gcd_cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;jitted_gpu_kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/*name=*/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gcd_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;cm&quot;&gt;/*return_dtype=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;cm&quot;&gt;/*common_dtype=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;cm&quot;&gt;/*arity=*/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gcd_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel&lt;/code&gt; is the entry point for the JITerator. It takes the name of the kernel, return dtype, computation dtype and the number of input arguments as template parameter. It also takes the TensorIterator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iter&lt;/code&gt; and the string for the computation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcd_string&lt;/code&gt; as run-time argument. At this point we are done with actually implementing the operator. When the operator is called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel&lt;/code&gt; will take care of compiling the kernel and loading it for use.&lt;/p&gt;

&lt;h3 id=&quot;jitted_gpu_kernel&quot;&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a383d01774beb112e519ae6a5c560eb402c96a31/aten/src/ATen/native/cuda/Loops.cuh#L111-L113&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;As mentioned above, this is the entry point for JITerator. The relevant code is present in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aten/src/ATen/native/cuda/Loops.cuh&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Following is a simplified C++ pseudo-code&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Entrypoint for jitted GPU kernels.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// Only handles elementwise unary and binary kernels with a&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//   common dtype and a single output.&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// NOTE: this assumes the op's iterator has a common_dtype.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;return_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;f_inputs_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;jitted_gpu_kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorIteratorBase&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NoScalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opmath_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_inputs_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Checks&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;can_use_32bit_indexing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;with_32bit_indexing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;jitted_gpu_kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_inputs_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Computes if dynamic casting is needed&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;needs_dynamic_casting&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// based on input and output dtype determine&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// if dynamic casting is needed&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// call `jitted_gpu_kernel_impl`&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NoScalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jitted_gpu_kernel_impl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/*name*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/*return_type=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/*f_inputs_type=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_inputs_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NoScalar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;needs_dynamic_casting&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RhsScalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jitted_gpu_kernel_impl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/*name*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/*return_type=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/*f_inputs_type=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_inputs_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RhsScalar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;needs_dynamic_casting&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jitted_gpu_kernel_impl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/*name*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/*return_type=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/*f_inputs_type=*/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_inputs_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LhsScalar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;needs_dynamic_casting&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function has some checks on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;output&lt;/code&gt; and figures out if we need the kernel requires dynamic casting. If the input size larger than one that can be handled by 32-bit indexing, we divide the input and output into 32-bit indexable blocks and call this function recursively. Post this the function calls &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel_impl&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;jitted_gpu_kernel_impl&quot;&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a383d01774beb112e519ae6a5c560eb402c96a31/aten/src/ATen/native/cuda/CUDALoops.cuh#L279-L281&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel_impl&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This function is located in&lt;/p&gt;

&lt;p&gt;Below is the actual C++ code&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;result_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;compute_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BinaryFuncVariant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NoScalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;jitted_gpu_kernel_impl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorIteratorBase&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dynamic_casting&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;TORCH_INTERNAL_ASSERT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;can_use_32bit_indexing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;TORCH_INTERNAL_ASSERT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ninputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;TORCH_INTERNAL_ASSERT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noutputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// get data pointers to input and output array.&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;constexpr&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ntensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ntensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;decltype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ntensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;kt&quot;&gt;int64_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Decides which of 4 kernel types to launch&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Variations are:&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//   - Case 1: no dynamic casting and contiguous&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//   - Case 2: no dynamic casting and noncontiguous&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//   - Case 3: dynamic casting and contiguous&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//   - Case 4: dynamic casting and noncontiguous&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// These cases align with the non-jitted CUDALoops.cuh cases in gpu_kernel_impl&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dynamic_casting&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;// Case 1: no dynamic casting and contiguous&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;launch_jitted_vectorized_kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Case 2: no dynamic casting and noncontiguous&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_offset_calculator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_input_offset_calculator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_offset_calculator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_output_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LoadWithoutCast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StoreWithoutCast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;launch_jitted_unrolled_kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;output_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Cases 3 and 4 are handled below&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Both require construction of a storer (this asserts 1 output) and one or more loaders&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Creates store cast to output (the zeroth tensor in TensorIterator)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StoreWithCast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Creates load casts from inputs (note offset indexing into the iterators 1...n tensors)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ScalarType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtypes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;decltype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dtypes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LoadWithCast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtypes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Case 3: dynamic casting and contiguous&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_offset_calculator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrivialOffsetCalculator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_offset_calculator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrivialOffsetCalculator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;launch_jitted_unrolled_kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;output_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Case 4: dynamic casting and noncontiguous&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_offset_calculator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_input_offset_calculator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_offset_calculator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make_output_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;launch_jitted_unrolled_kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_pos&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_offset_calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contiguous&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above function all it does if figure out the memory layout of the input and output tensor and adds extra machinery if they are non-contiguous and if dynamic casting is required (computed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jitted_gpu_kernel&lt;/code&gt;). Post that it passes the computed data to either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_unrolled_kernel&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_vectorized_kernel&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;nvrtc-jit-utility-helpers&quot;&gt;NVRTC JIT utility helpers&lt;/h3&gt;

&lt;p&gt;Before we look at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_vectorized_kernel&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_unrolled_kernel&lt;/code&gt;, let’s take a quick understanding at the JIT utility that they call.&lt;/p&gt;

&lt;p&gt;These utility functions are declared in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aten/src/ATen/native/cuda/jit_utils.h&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/jit_utils.h&quot;&gt;file&lt;/a&gt; and defined in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aten/src/ATen/native/cuda/jit_utils.cu&lt;/code&gt; &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/jit_utils.cu&quot;&gt;file&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Important utility helpers to know about&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;generate_code&lt;/code&gt; : This function takes the computation string and wraps it in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt; with the required machinery of reading from input, passing that data to kernel and writing to the output. It also returns a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jit_vectorized_code_template&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jit_code_template&lt;/code&gt; : These strings are used to wrap the actual computation string to create the actual kerel. These take care of efficiently loading and storing the input and output tensor and calling our computation code on them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jit_pwise_function&lt;/code&gt;: This function takes in the string generated from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;generate_code&lt;/code&gt; and kernel name. It uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NVRTC&lt;/code&gt; functions to compile the code and load that compiled code into the GPU VRAM and return pointer to the kernel function pointer for calling.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_pwise_function&lt;/code&gt; : This function takes the kernel pointer we got from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jit_pwise_function&lt;/code&gt; with the data to be passed to the kernel and launch the kernel using the function provided by cuda-toolkit to launch this runtime compiled kernel.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;launch_jitted_unrolled_kernel-and-launch_jitted_vectorized_kernel&quot;&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a383d01774beb112e519ae6a5c560eb402c96a31/aten/src/ATen/native/cuda/CUDALoops.cuh#L126-L135&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_unrolled_kernel&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/a383d01774beb112e519ae6a5c560eb402c96a31/aten/src/ATen/native/cuda/CUDALoops.cuh#L179-L186&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_vectorized_kernel&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Besides the using the machinery of consuming vectorized code these functions do identical task. So we will just look at one of them and interested people can look at the other one.&lt;/p&gt;

&lt;p&gt;We will look at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_vectorized_kernel&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;template&amp;lt;
  char const *name,
  typename result_type,
  typename f_inputs_type,
  int arity,
  at::cuda::jit::BinaryFuncVariant scalar_pos,
  typename array_t&amp;gt;
static inline void launch_jitted_vectorized_kernel(DeviceIndex dev_idx, int64_t N, const std::string&amp;amp; f, array_t data,
at::opmath_type&amp;lt;f_inputs_type&amp;gt; scalar_val) {
  TORCH_INTERNAL_ASSERT(N &amp;gt; 0 &amp;amp;&amp;amp; N &amp;lt;= std::numeric_limits&amp;lt;int32_t&amp;gt;::max());
  const int64_t grid = (N + block_work_size() - 1) / block_work_size();
  const int vec_size = memory::jitted_can_vectorize_up_to&amp;lt;result_type, f_inputs_type, arity&amp;gt;(data);

  // Different kernels are compiled depending on what we're vectorizing up to (1, 2 or 4 elements)
  //   fn_ptr is set to the appropriate function based on the vec size and GPU used
  // TODO: Memory use can probably be optimized by re-using kernels across GPUs with
  //   the same compute capability
  static std::mutex _jiterator_mutex;
  static std::vector&amp;lt;at::cuda::jit::NvrtcFunction&amp;gt; fns4(c10::cuda::device_count());
  static std::vector&amp;lt;at::cuda::jit::NvrtcFunction&amp;gt; fns2(c10::cuda::device_count());
  static std::vector&amp;lt;at::cuda::jit::NvrtcFunction&amp;gt; fns1(c10::cuda::device_count());


  at::cuda::jit::NvrtcFunction* fn_ptr;
  if (vec_size == 4) {
    fn_ptr = &amp;amp;fns4[dev_idx];
  } else if (vec_size == 2) {
    fn_ptr = &amp;amp;fns2[dev_idx];
  } else if (vec_size ==1) {
    fn_ptr = &amp;amp;fns1[dev_idx];
  } else {
    TORCH_INTERNAL_ASSERT(false, &quot;unexpected vec_size for jitter vectorized kernel&quot;);
  }

  bool vectorized = vec_size &amp;gt; 1;

  if (!fn_ptr-&amp;gt;function) {
    // generate code if fn_ptr-&amp;gt;function is nullptr
    // i.e. we haven't compiled it previously.
    const std::lock_guard&amp;lt;std::mutex&amp;gt; lock{_jiterator_mutex};
    if (!fn_ptr-&amp;gt;function) {
      constexpr int nTensors = array_t::size();
      std::string string_name{name};
      std::string f_inputs_type_str = at::cuda::jit::typeName&amp;lt;f_inputs_type&amp;gt;();
      std::string compute_type_str = at::cuda::jit::typeName&amp;lt;at::opmath_type&amp;lt;f_inputs_type&amp;gt;&amp;gt;();
      std::string result_type_str = at::cuda::jit::typeName&amp;lt;result_type&amp;gt;();
      auto code = at::cuda::jit::generate_code(nTensors, f, string_name,
                                               f_inputs_type_str, compute_type_str, result_type_str,
                                               /*contiguous=*/true, /*dynamic_casting=*/false,
                                               scalar_pos,
                                               vectorized, vec_size);
      std::string kernel_name = vectorized ? string_name + &quot;_vectorized&quot; + std::to_string(vec_size) : string_name;
      *fn_ptr = at::cuda::jit::jit_pwise_function(code, kernel_name);
    }
  }

  if (vectorized) {
    std::array&amp;lt;void*, 7&amp;gt; args = {
      (void*)&amp;amp;N,
      (void*)&amp;amp;data,
      (void*)&amp;amp;scalar_val,
      nullptr,
      nullptr,
      nullptr,
      nullptr
    };

    at::cuda::jit::launch_jitted_pwise_function(*fn_ptr, args, grid, num_threads());
    C10_CUDA_KERNEL_LAUNCH_CHECK();
  } else {
    auto ic = TrivialOffsetCalculator&amp;lt;arity&amp;gt;();
    auto oc = TrivialOffsetCalculator&amp;lt;1&amp;gt;();
    auto l = memory::LoadWithoutCast();
    auto s = memory::StoreWithoutCast();

    std::array&amp;lt;void*, 7&amp;gt; args = {
      (void*)&amp;amp;N,
      (void*)&amp;amp;data,
      (void*)&amp;amp;ic,
      (void*)&amp;amp;oc,
      (void*)&amp;amp;l,
      (void*)&amp;amp;s,
      (void*)&amp;amp;scalar_val
    };

    at::cuda::jit::launch_jitted_pwise_function(*fn_ptr, args, grid, num_threads());
    C10_CUDA_KERNEL_LAUNCH_CHECK();
  }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we enter the function we check our cache &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fns4/fns2/fns1&lt;/code&gt; to get the pointer to kernel function if it exists. If it doesn’t we use the utilities we looked at above. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;generate_code&lt;/code&gt; is called with our computation string to generate the actual kernel code.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jit_pwise_function&lt;/code&gt; is called to actually compile and get the function pointer to this kernel. Notice that we use the vectorized kernel if possible in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_vectorized_kernel&lt;/code&gt;. We update our cache pointer with to point to this kernel and use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;launch_jitted_pwsie_function&lt;/code&gt; from JIT utility to launch this kernel with the input and output. Once the kernel is launched, the computation happens on GPU and we get the output and user is happy.&lt;/p&gt;

&lt;p&gt;On next run of the operator, the pointer from the cache will be valid and we will directly use it instead of compiling the code again.&lt;/p&gt;

&lt;h3 id=&quot;limitations-of-jiterator&quot;&gt;Limitations of JITerator&lt;/h3&gt;

&lt;p&gt;There is a &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/69463&quot;&gt;tracking issue&lt;/a&gt; to track the limitations and improve them.&lt;/p&gt;

&lt;p&gt;We will talk about a few here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;No math ops on complex dtypes : This means that the operator which support complex data can only be partially JITerated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compile Operator for every individual PyTorch run : Curious and observing reader must have noticed that the cache is a static variable and we don’t write the compiled kernel anywhere on the file-system. So every time you load PyTorch and use one of the JITerated operator, you end up compiling them on the first run.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can’t capture runtime state (supported by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpu_kernel&lt;/code&gt;) : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpu_kernel&lt;/code&gt; allows us to capture some runtime state, eg. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int n&lt;/code&gt; passed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.polygamma&lt;/code&gt;. Right now this is not supported by JITerator.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Kshiteej</name><email>kshitijkalambarkar@gmail.com</email></author><category term="pytorch," /><category term="cuda" /><summary type="html">PyTorch is one of the leading Deep Learning framework. It supports a lot of operations to operate on Tensor. This rich set of operators allow researchers to quickly proto-type new solutions to their problems. As these operators are the basic building block of any PyTorch script, it is imperative that they are as performant as possible. Writing performant operators with parallelization while keeping the cache hot is not easy. Writing these operator considering that the Tensors are not always laid out contiguously in memory, is certainly tricky. Handling these myraid of cases while maintaining performance does not sounds like a non-trivial development task.</summary></entry><entry><title type="html">Leadership</title><link href="https://kshitij12345.github.io/leadership,/management/2021/10/10/Leadership.html" rel="alternate" type="text/html" title="Leadership" /><published>2021-10-10T14:14:49+05:30</published><updated>2021-10-10T14:14:49+05:30</updated><id>https://kshitij12345.github.io/leadership,/management/2021/10/10/Leadership</id><content type="html" xml:base="https://kshitij12345.github.io/leadership,/management/2021/10/10/Leadership.html">&lt;h3 id=&quot;leadership&quot;&gt;Leadership&lt;/h3&gt;

&lt;h4 id=&quot;what-is-leadership&quot;&gt;What is leadership?&lt;/h4&gt;

&lt;p&gt;Leadership can be very simply defined as making the decision and implementing those decision. To accomplish this
you need to also be able to influence people (around/below/above) you.&lt;/p&gt;

&lt;h4 id=&quot;leadership-vs-management&quot;&gt;Leadership vs Management&lt;/h4&gt;

&lt;p&gt;Leadership is not necessarily about the authority or the title that a person has. 
A person who is a manager maybe or may not be a leader in the sense of above definition.
Similarly, a person who does not lead a team can be a leader. It is more about the leadership
skills than mere position.&lt;/p&gt;

&lt;h4 id=&quot;qualities-of-a-leader&quot;&gt;Qualities of a Leader&lt;/h4&gt;

&lt;p&gt;This &lt;a href=&quot;https://hbr.org/2016/03/the-most-important-leadership-competencies-according-to-leaders-around-the-world&quot;&gt;blog&lt;/a&gt; talks about the skills that
a good leader has or should focus on. The skills are ranked based on the opinion of 195 leaders from around the world and different companies.
One that stood out to me was openness. Being able to admit, I am wrong and being able to improve upon that is really a skill. It is not easy to
accept one’s mistakes especially if someone lower than your current title points them out.&lt;/p&gt;

&lt;h4 id=&quot;inclusiveness&quot;&gt;Inclusiveness&lt;/h4&gt;

&lt;p&gt;Another quality is being inclusive. Being inclusive, is prior requirement for being able to admit one’s mistakes. What I mean by being inclusive here
is that the person should try to include people while making and implementing a decision. Since, it is not possible to talk to every person, the leader
should target to atleast to the sample of people which represent all the groups. The idea is to look past one’s perspective and to look at the bigger picture
together. Including people also has the benefit that people feel responsible and ownership about the decision and strive harder to make sure that it is
implemented successfuly.&lt;/p&gt;

&lt;p&gt;There is more to be a leader than this, but that’s it for today!!&lt;/p&gt;</content><author><name>Kshiteej</name><email>kshitijkalambarkar@gmail.com</email></author><category term="leadership," /><category term="management" /><summary type="html">Leadership</summary></entry><entry><title type="html">GPU Parallel Programming: Chapter 1</title><link href="https://kshitij12345.github.io/c++,/cuda/2021/08/07/GPU-Parallel-Chapter-1.html" rel="alternate" type="text/html" title="GPU Parallel Programming: Chapter 1" /><published>2021-08-07T14:14:49+05:30</published><updated>2021-08-07T14:14:49+05:30</updated><id>https://kshitij12345.github.io/c++,/cuda/2021/08/07/GPU-Parallel-Chapter-1</id><content type="html" xml:base="https://kshitij12345.github.io/c++,/cuda/2021/08/07/GPU-Parallel-Chapter-1.html">&lt;h3 id=&quot;chapter-1-introduction-to-cpu-parallel-programming&quot;&gt;Chapter 1: Introduction to CPU Parallel Programming&lt;/h3&gt;

&lt;p&gt;The promise of Moore’s Law is dead (though not the Moore’s Law itself). The number of transistor’s on a chip are
increasing every 2 years, but the frequency increase has hit wall due to power consumption and heating issues.
So these new chips have more cores and for now it’s upto the programmers to get the best out of these multi-core systems.&lt;/p&gt;

&lt;h4 id=&quot;more-core-doesnt-directly-mean-more-performance&quot;&gt;More core doesn’t directly mean more performance&lt;/h4&gt;

&lt;p&gt;It is imperative to understand the fact that just because you make your program parallel doesn’t mean, it will be faster.
It is important to correctly orchestrate the work threads are doing. Also, with the fact the memory access being slower,
sometimes your program can get memory bound i.e. your cores are being underutilized and waiting for the data to be ready.
So parallel programming is mmore than just throwing more threads at your problem (unless it’s embarassingly parallel).&lt;/p&gt;

&lt;h4 id=&quot;data-bandwith-for-different-devies&quot;&gt;Data-bandwith for different devies&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Network Interface Card : 1Gbps (Gigabits per second)&lt;/li&gt;
  &lt;li&gt;HDD connected over PCI3 bus : 1-2 Gbps (6Gbps max possible)&lt;/li&gt;
  &lt;li&gt;USB 3 : Max 10Gbps&lt;/li&gt;
  &lt;li&gt;SSD over PCI3 bus : 4-5 Gbps (6Gbps max possible)&lt;/li&gt;
  &lt;li&gt;RAM : 20-60 GBps (Gigabytes per second) / 160-480 Gbps&lt;/li&gt;
  &lt;li&gt;GPU Internal Memory : 10-1000 GBps&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;interesting-note-from-the-book&quot;&gt;Interesting Note from the book&lt;/h4&gt;

&lt;p&gt;We can get less noisy performance/benchmark data if our data fits in cache because once data is in cache the execution
is fairly deterministic. However if data spills over the cache, then due to non-deterministic nature of memory access
from RAM (due to other programs running on OS and OS overheads) the benchmark data will be more noisy!&lt;/p&gt;</content><author><name>Kshiteej</name><email>kshitijkalambarkar@gmail.com</email></author><category term="c++," /><category term="cuda" /><summary type="html">Chapter 1: Introduction to CPU Parallel Programming</summary></entry><entry><title type="html">Modern Effective C++: Chapter 2</title><link href="https://kshitij12345.github.io/c++/2021/07/17/Modern-Effective-C++-Chapter-2.html" rel="alternate" type="text/html" title="Modern Effective C++: Chapter 2" /><published>2021-07-17T14:14:49+05:30</published><updated>2021-07-17T14:14:49+05:30</updated><id>https://kshitij12345.github.io/c++/2021/07/17/Modern-Effective-C++-Chapter-2</id><content type="html" xml:base="https://kshitij12345.github.io/c++/2021/07/17/Modern-Effective-C++-Chapter-2.html">&lt;h3 id=&quot;chapter-2--auto&quot;&gt;Chapter 2 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt;&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;Item 5: Prefer auto to explicit type declarations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; whenever possible (helps you from explicitly adding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;types&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Also, it helps avoid few errors
    &lt;ol&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; doesn’t support uninitialized variables. Unlike &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int x&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; is uninitialized, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto x;&lt;/code&gt; is not valid! It has to be either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto x{1}&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto x = 1;&lt;/code&gt; (1 is just some random value in this example)&lt;/li&gt;
      &lt;li&gt;It helps to easily take get the value type of iterator objects. Replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typename std::iterator_traits&amp;lt;It&amp;gt;::value_type
 currValue = *b;&lt;/code&gt; -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto currValue = *b&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;For entities like lambda, their type is only known to compiler so how on earth can you make a variable for it using explicit type? &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; to the rescue 😉&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Also helps avoid subtle bugs (example: unsigned len = vec.size(), return type of vec.size() is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&amp;lt;T&amp;gt;::size_type&lt;/code&gt;, however  since unsigned is platform dependent it can lead to bugs if it doesn’t match &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&amp;lt;T&amp;gt;::size_type&lt;/code&gt;) (Refer example on page 40 which talks about vec.size() )&lt;/li&gt;
  &lt;li&gt;Can avoid temporary object creation (Refer example on page 40 which talks about unordered map)&lt;/li&gt;
  &lt;li&gt;For people worried about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; decreasing the readability should note that tons of succesful language have type inference where user doesn’t need to explicitly specify the types. Also IDE and editor can/should help reveal the type.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Item 6:  Use the explicitly typed initializer idiom when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; deduces undesired types.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For some edge-cases, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; may deduce type which you don’t actually want, in that case you should use the &lt;strong&gt;explicitly type initializer&lt;/strong&gt; idiom. Eg. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto x = static_cast&amp;lt;float&amp;gt;(get_double_eps())&lt;/code&gt;, this approach is better than implicitly converting to float using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float x = get_double_eps()&lt;/code&gt;. Also this will help you to get around the functions or interface which return &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proxy&lt;/code&gt; object (as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; will deduce the type to be proxy object and not the type it is acting as proxy for). Proxy objects are those which behave like a certain type but are not of that type. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector::operator[]&lt;/code&gt; returns proxy object (as reference to bits is not valid). (Refer to Item 6 in book for gory details 😄 )&lt;/li&gt;
  &lt;li&gt;Prefer explicit cast over implicit cast!&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Kshiteej</name><email>kshitijkalambarkar@gmail.com</email></author><category term="c++" /><summary type="html">Chapter 2 : auto Item 5: Prefer auto to explicit type declarations. Use auto whenever possible (helps you from explicitly adding the types) Also, it helps avoid few errors auto doesn’t support uninitialized variables. Unlike int x where x is uninitialized, auto x; is not valid! It has to be either auto x{1} or auto x = 1; (1 is just some random value in this example) It helps to easily take get the value type of iterator objects. Replace typename std::iterator_traits&amp;lt;It&amp;gt;::value_type currValue = *b; -&amp;gt; auto currValue = *b For entities like lambda, their type is only known to compiler so how on earth can you make a variable for it using explicit type? auto to the rescue 😉 Also helps avoid subtle bugs (example: unsigned len = vec.size(), return type of vec.size() is std::vector&amp;lt;T&amp;gt;::size_type, however since unsigned is platform dependent it can lead to bugs if it doesn’t match std::vector&amp;lt;T&amp;gt;::size_type) (Refer example on page 40 which talks about vec.size() ) Can avoid temporary object creation (Refer example on page 40 which talks about unordered map) For people worried about auto decreasing the readability should note that tons of succesful language have type inference where user doesn’t need to explicitly specify the types. Also IDE and editor can/should help reveal the type. Item 6: Use the explicitly typed initializer idiom when auto deduces undesired types. For some edge-cases, auto may deduce type which you don’t actually want, in that case you should use the explicitly type initializer idiom. Eg. auto x = static_cast&amp;lt;float&amp;gt;(get_double_eps()), this approach is better than implicitly converting to float using float x = get_double_eps(). Also this will help you to get around the functions or interface which return proxy object (as auto will deduce the type to be proxy object and not the type it is acting as proxy for). Proxy objects are those which behave like a certain type but are not of that type. std::vector::operator[] returns proxy object (as reference to bits is not valid). (Refer to Item 6 in book for gory details 😄 ) Prefer explicit cast over implicit cast!</summary></entry><entry><title type="html">Contributing to Open Source Software (OSS) (Slide Deck)</title><link href="https://kshitij12345.github.io/oss/2021/04/29/contributing-to-oss-slide.html" rel="alternate" type="text/html" title="Contributing to Open Source Software (OSS) (Slide Deck)" /><published>2021-04-29T14:14:49+05:30</published><updated>2021-04-29T14:14:49+05:30</updated><id>https://kshitij12345.github.io/oss/2021/04/29/contributing-to-oss-slide</id><content type="html" xml:base="https://kshitij12345.github.io/oss/2021/04/29/contributing-to-oss-slide.html">&lt;h4 id=&quot;slide-preview&quot;&gt;Slide Preview&lt;/h4&gt;

&lt;p&gt;HackMD &lt;a href=&quot;https://hackmd.io/@kshiteejk/SJc65ipL_&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;contributing-to-open-source-software-oss&quot;&gt;Contributing to Open Source Software (OSS)&lt;/h4&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;who-am-i&quot;&gt;Who Am I?&lt;/h4&gt;

&lt;p&gt;&lt;span style=&quot;font-size:0.75em;&quot;&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Software Engineer at Quansight (developing PyTorch)&lt;/li&gt;
  &lt;li&gt;Contributed to PyTorch, MXNet, Chainer, etc.&lt;/li&gt;
  &lt;li&gt;Previously Machine Learning Engineer (in NLP and CV)&lt;/li&gt;
  &lt;li&gt;Not from CS background (only language university taught us was C 😔 )&lt;/li&gt;
  &lt;li&gt;Learnt Python by wandering in the wild!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;lt;/span&amp;gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;what-is-oss&quot;&gt;What is OSS?&lt;/h3&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;brief-history&quot;&gt;Brief History&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Richard Stallman (pioneer)&lt;/li&gt;
  &lt;li&gt;Sharing your recipe&lt;/li&gt;
  &lt;li&gt;Imagine world without OSS 😨&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;does-anyone-even-use-it&quot;&gt;Does anyone even use it?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Recent Mars Ingenuity Helicopter&lt;/li&gt;
  &lt;li&gt;If you use Python&lt;/li&gt;
  &lt;li&gt;If you are using any cloud service&lt;/li&gt;
  &lt;li&gt;If you use Android&lt;/li&gt;
  &lt;li&gt;If you use ….&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;why-should-i-spend-my-time&quot;&gt;Why should I spend my time?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;People will use your code! 😁&lt;/li&gt;
  &lt;li&gt;Learning 📜&lt;/li&gt;
  &lt;li&gt;Ability to work in a team and interact! 🤝&lt;/li&gt;
  &lt;li&gt;Meet new people! 👩‍💻&lt;/li&gt;
  &lt;li&gt;Get introduced to new tools and technologies (debuggers, linters)🔧&lt;/li&gt;
  &lt;li&gt;Might land a new job 💻&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;hopefully-you-are-interested-by-now-&quot;&gt;Hopefully you are interested by now 🤓&lt;/h4&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;start-slow&quot;&gt;Start Slow&lt;/h4&gt;

&lt;hr /&gt;

&lt;p&gt;My experience&lt;/p&gt;

&lt;p&gt;First Pytorch PR terrible 🤢&lt;/p&gt;

&lt;p&gt;Took 9 months to get accepted!
Lots of rookie mistakes!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Start Date&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/s7nRdXB.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;End Date 🎉&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/socFCiV.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;experience&quot;&gt;Experience&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Patience&lt;/li&gt;
  &lt;li&gt;Lot of learning&lt;/li&gt;
  &lt;li&gt;Got to interact with &lt;strong&gt;really&lt;/strong&gt; smart people&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;tips&quot;&gt;Tips&lt;/h2&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;choosing-the-project-to-work-on&quot;&gt;Choosing the project to work on&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Personal Project that you really love&lt;/li&gt;
  &lt;li&gt;Some library that you use a lot (eg. NumPy, pandas)&lt;/li&gt;
  &lt;li&gt;Be sure to choose an active project&lt;/li&gt;
  &lt;li&gt;Project with developer guide and good documentation&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;things-to-remember&quot;&gt;Things to remember&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Ask for help! Nobody expects you to know everything&lt;/li&gt;
  &lt;li&gt;Be polite, humble and &lt;strong&gt;patient&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;There is no stupid question!&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;thank-you&quot;&gt;Thank You!&lt;/h4&gt;

&lt;p&gt;Github Handle: kshitij12345
LinkedIn: kshiteejkalambarkar&lt;/p&gt;</content><author><name>Kshiteej</name><email>kshitijkalambarkar@gmail.com</email></author><category term="oss" /><summary type="html">Slide Preview</summary></entry><entry><title type="html">Gentle Introduction to Valgrind!</title><link href="https://kshitij12345.github.io/intro/2021/04/29/valgrind-intro.html" rel="alternate" type="text/html" title="Gentle Introduction to Valgrind!" /><published>2021-04-29T14:14:49+05:30</published><updated>2021-04-29T14:14:49+05:30</updated><id>https://kshitij12345.github.io/intro/2021/04/29/valgrind-intro</id><content type="html" xml:base="https://kshitij12345.github.io/intro/2021/04/29/valgrind-intro.html">&lt;h2 id=&quot;valgrind&quot;&gt;Valgrind&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.valgrind.org/downloads/&quot;&gt;Valgrind&lt;/a&gt; is an useful tool if you are working with C/C++. It is a suite of multipurpose tools with varied functionality. It allows you to detect memory-leak, profile your code and more. One of the thing that it shines at is helping debugging trick memory bug.&lt;/p&gt;

&lt;h3 id=&quot;how-does-it-work&quot;&gt;How does it work?&lt;/h3&gt;
&lt;p&gt;It is worth noting that Valgrind is language agnostic and works with all languages, compiled or interpreted. Since Valgrind consumes a binary, it does not care about which language it came from. Valgrind is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;non-intrusive&lt;/code&gt; in terms of adding an instrumentation code. You need not update your codebase for it to work with Valgrind. The way it manages to do this is by providing a virtual core for the binary to run on. This way it can also figure out statistics like number of number native instructions executed.&lt;/p&gt;

&lt;h3 id=&quot;installing-valgrind&quot;&gt;Installing Valgrind&lt;/h3&gt;
&lt;p&gt;Let’s start with setuping up a playground environment with Valgrind&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$conda&lt;/span&gt; create &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; valgrind-test-env
&lt;span class=&quot;nv&quot;&gt;$conda&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; conda-forge compilers
&lt;span class=&quot;nv&quot;&gt;$conda&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; conda-forge valgrind
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;example-program&quot;&gt;Example Program&lt;/h3&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This innocuous looking program is actually ill-formed. The reason being, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new[]&lt;/code&gt; allocation the allocated memory should be freed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delete[]&lt;/code&gt;, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;delete&lt;/code&gt; the behavior is undefined.&lt;/p&gt;

&lt;h3 id=&quot;building-the-program&quot;&gt;Building the program&lt;/h3&gt;
&lt;p&gt;Let’s build with most of the compiler warnings enabled to see if our compiler can foresee this issue and warn us.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;g++ example.cc &lt;span class=&quot;nt&quot;&gt;-Wall&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-Wpedantic&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-Werror&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-Wextra&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On running the following, we get a shiny new executable without any warnings.&lt;/p&gt;

&lt;h3 id=&quot;running-under-valgrind&quot;&gt;Running under Valgrind&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$valgrind --tool=memcheck ./a.out
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we are asking Valgrind to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memcheck&lt;/code&gt; on our binary. There are more tools and options which can be specified.&lt;/p&gt;

&lt;p&gt;On running the above command, we get the following output.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;==19750== Memcheck, a memory error detector
==19750== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==19750== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info
==19750== Command: ./a.out
==19750== 
==19750== Mismatched free() / delete / delete []
==19750==    at 0x403713B: operator delete(void*, unsigned long) (vg_replace_malloc.c:595)
==19750==    by 0x1091AD: main (in /home/user/Desktop/Repositories/valgrind-test/a.out)
==19750==  Address 0x51ccc80 is 0 bytes inside a block of size 10 alloc'd
==19750==    at 0x40365AF: operator new[](unsigned long) (vg_replace_malloc.c:433)
==19750==    by 0x109193: main (in /home/user/Desktop/Repositories/valgrind-test/a.out)
==19750== 
==19750== 
==19750== HEAP SUMMARY:
==19750==     in use at exit: 0 bytes in 0 blocks
==19750==   total heap usage: 2 allocs, 2 frees, 72,714 bytes allocated
==19750== 
==19750== All heap blocks were freed -- no leaks are possible
==19750== 
==19750== For lists of detected and suppressed errors, rerun with: -s
==19750== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Mismatched free() / delete / delete []&lt;/code&gt;. The message tells us that we have a mismatched &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;free() / delete / delete []&lt;/code&gt; for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new[]&lt;/code&gt;. Valgrind is able to detect this as it does the booking keeping of the instruction it executed, instructions which requested memory and instructions which freed it.&lt;/p&gt;

&lt;h2 id=&quot;caveats&quot;&gt;Caveats&lt;/h2&gt;
&lt;p&gt;Since Valgrind emulates the hardware and does more tracking and bookkeeping, running a program under Valgrind is much slower.&lt;/p&gt;

&lt;p&gt;Valgrind’s site states&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;So what's the catch? The main one is that programs run significantly more slowly under Valgrind. Depending on which tool you use, the slowdown factor can range from 5--100. This slowdown is similar to that of similar debugging and profiling tools. But since you don't have to use Valgrind all the time, this usually isn't too much of a problem. The hours you'll save debugging will more than make up for it.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;epilogue&quot;&gt;Epilogue&lt;/h2&gt;

&lt;p&gt;All in all, Valgrind is worthwhile tool to catch bugs. Even though it is slow, it will cut down on the debugging time by a large margin.&lt;/p&gt;</content><author><name>Kshiteej</name><email>kshitijkalambarkar@gmail.com</email></author><category term="intro" /><summary type="html">Valgrind Valgrind is an useful tool if you are working with C/C++. It is a suite of multipurpose tools with varied functionality. It allows you to detect memory-leak, profile your code and more. One of the thing that it shines at is helping debugging trick memory bug.</summary></entry></feed>